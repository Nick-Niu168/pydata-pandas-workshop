{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Königsweg Logo](../img/koenigsweg_150.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: small;float: right;\">&copy; 2015-2020 Alexander C.S. Hendorf, <a href=\"http://koenigsweg.com\">Königsweg GmbH</a>, Mannheim </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics with  Pandas and Jupyterlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and Optimizing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import parquet\n",
    "import dask\n",
    "%matplotlib inline\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_file_path = '../data/blooth_sales_data_big.json'  # 42 MB json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catagorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you deal with table with a lot of repetive data, a Categorical can ge a good option to save space. It's basically a lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_big_set = pd.read_json(large_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_big_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_big_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_big_set.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_big_set.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_big_set.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_big_set['product'] = tiny_big_set['product'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_big_set.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Parquet is a\n",
    "* free and open-source column-oriented data store of the Apache Hadoop ecosystem.\n",
    "* top-level Apache Software Foundation (ASF)-sponsored project.\n",
    "* built from the ground up with complex nested data structures in mind\n",
    "\n",
    "Benefits:\n",
    "* Column-wise compression is efficient and saves storage space\n",
    "* Compression techniques specific to a type can be applied as the column values tend to be of the same type\n",
    "* Queries that fetch specific column values need not read the entire row data thus improving performance\n",
    "* Different encoding techniques can be applied to different columns\n",
    "* can work with a number of programming languages like C++, Java, Python, PHP,…\n",
    "* lower data storage costs and maximize effectiveness of querying data (e.g. with serverless technologies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.utcnow()\n",
    "df = pd.read_json(large_file_path)\n",
    "took = start = dt.utcnow() - start\n",
    "took.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f'{large_file_path}.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.utcnow()\n",
    "df = pd.read_parquet(f'{large_file_path}.parquet.gzip')\n",
    "took = start = dt.utcnow() - start\n",
    "took.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask natively scales Python.\n",
    "\n",
    "Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love as \n",
    "* Pandas\n",
    "* Numpy\n",
    "* Scikit-Learn\n",
    "\n",
    "We can summarize the basics of Dask as follows:\n",
    "* process data that doesn't fit into memory by breaking it into blocks and specifying task chains\n",
    "* parallelize execution of tasks across cores and even nodes of a cluster\n",
    "* move computation to the data rather than the other way around, to minimize communication overheads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing articial data\n",
    "df['total'] = df.units * df.unitprice\n",
    "for i in range(5):\n",
    "    df.to_csv(f'/tmp/data_for_dask_{i}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is great for tabular datasets that fit in memory. \n",
    "Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM. \n",
    "\n",
    "The dask.dataframe module implements a blocked parallel DataFrame object that mimics a large subset of the Pandas DataFrame. One Dask DataFrame is comprised of many in-memory pandas DataFrames separated along the index. One operation on a Dask DataFrame triggers many pandas operations on the constituent pandas DataFrames in a way that is mindful of potential parallelism and memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "filename = f'/tmp/data_for_dask_*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(filename)\n",
    "# load and count number of rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Pandas** way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.utcnow()\n",
    "\n",
    "maxes = []\n",
    "for fn in [f'/tmp/data_for_dask_{i}.csv' for i in range(5)]:\n",
    "    pdf = pd.read_csv(fn)\n",
    "    maxes.append(pdf.total.max())\n",
    "    \n",
    "took = start = dt.utcnow() - start\n",
    "took.total_seconds(), max(maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dask** way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.utcnow()\n",
    "\n",
    "df.total.max().compute()\n",
    "\n",
    "took = start = dt.utcnow() - start\n",
    "took.total_seconds(), max(maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dask](../img/dask-compute.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
